#!/usr/bin/env python3
"""
Train a custom Byte-Level BPE tokenizer for T5-Nano.

Reads:   data/processed/corpus.txt
Writes:  custom_tokenizer/vocab.json and custom_tokenizer/merges.txt
"""

from __future__ import annotations

import argparse
from pathlib import Path


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--corpus_path",
        type=Path,
        default=Path("data/processed/corpus.txt"),
        help="Path to corpus.txt generated by data_prep.py",
    )
    ap.add_argument(
        "--output_dir",
        type=Path,
        default=Path("custom_tokenizer"),
        help="Directory to write vocab.json + merges.txt",
    )
    ap.add_argument(
        "--vocab_size",
        type=int,
        default=16_000,
        help="Vocabulary size (must be 16000 per spec)",
    )
    args = ap.parse_args()

    if not args.corpus_path.exists():
        raise FileNotFoundError(f"Corpus file not found: {args.corpus_path}")

    args.output_dir.mkdir(parents=True, exist_ok=True)

    # Train Byte-Level BPE tokenizer
    from tokenizers import ByteLevelBPETokenizer

    special_tokens = ["<s>", "<pad>", "</s>", "<unk>", "<mask>"]
    tokenizer = ByteLevelBPETokenizer()
    tokenizer.train(
        files=[str(args.corpus_path)],
        vocab_size=int(args.vocab_size),
        special_tokens=special_tokens,
    )

    tokenizer.save_model(str(args.output_dir))

    vocab_path = args.output_dir / "vocab.json"
    merges_path = args.output_dir / "merges.txt"
    if not vocab_path.exists() or not merges_path.exists():
        raise RuntimeError(
            f"Tokenizer training did not produce expected files in {args.output_dir}"
        )

    print(
        f"Success: trained Byte-Level BPE tokenizer (vocab_size={args.vocab_size}) "
        f"and saved to {args.output_dir}"
    )
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

