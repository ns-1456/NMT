# English-Gujarati NMT Configuration

# Paths
paths:
  data_dir: "data"
  raw_data_dir: "data/raw"
  processed_data_dir: "data/processed"
  splits_dir: "data/splits"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"

# Dataset configuration
dataset:
  source_lang: "en"
  target_lang: "gu"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_length: 128
  min_length: 3
  max_sentences: null  # null means use all available

# Tokenization
tokenization:
  type: "bpe"  # "bpe" or "unigram"
  vocab_size: 16000
  source_vocab_size: 16000
  target_vocab_size: 16000
  # For comparison experiments
  vocab_sizes_to_test: [4000, 8000, 16000, 32000]

# Model architecture - Student (Small Transformer <50M params)
model:
  student:
    num_layers: 4
    d_model: 512
    num_heads: 8
    d_ff: 2048
    dropout: 0.1
    max_seq_length: 128
    # Parameter count should be <50M
    
  # Teacher model (for knowledge distillation)
  teacher:
    num_layers: 6
    d_model: 512
    num_heads: 8
    d_ff: 2048
    dropout: 0.1
    max_seq_length: 128

# Training configuration
training:
  batch_size: 32
  gradient_accumulation_steps: 1
  num_epochs: 50
  learning_rate: 1e-4
  warmup_steps: 4000
  weight_decay: 0.01
  max_grad_norm: 1.0
  save_steps: 5000
  eval_steps: 1000
  logging_steps: 100
  save_total_limit: 3
  seed: 42

# Knowledge Distillation
distillation:
  enabled: true
  temperature: 4.0
  alpha: 0.5  # Weight for hard loss: alpha * hard_loss + (1-alpha) * soft_loss
  teacher_checkpoint: null  # Path to teacher model checkpoint

# Evaluation
evaluation:
  beam_size: 5
  max_length: 128
  length_penalty: 0.6
  no_repeat_ngram_size: 3

# Device
device: "cuda"  # "cuda" or "cpu"

# Logging
logging:
  use_wandb: false
  wandb_project: "en-gu-nmt"
  wandb_entity: null
