{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# English-Gujarati NMT: Data Preparation\n",
        "\n",
        "This notebook prepares the dataset for training the NMT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install transformers tokenizers sentencepiece datasets sacrebleu pyyaml tqdm wandb requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository or upload files\n",
        "# Option 1: Clone from GitHub\n",
        "# !git clone https://github.com/ns-1456/NMT.git\n",
        "# %cd NMT\n",
        "\n",
        "# Option 2: Upload files manually\n",
        "# Upload the entire project folder to Colab\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and prepare dataset\n",
        "from src.data.download import prepare_dataset\n",
        "from src.data.preprocess import preprocess_parallel_files\n",
        "from src.data.dataset import create_data_splits\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Downloading FLORES dataset...\")\n",
        "en_file, gu_file = prepare_dataset(data_dir, dataset_name=\"flores\")\n",
        "print(f\"Downloaded: {en_file}, {gu_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "print(\"Preprocessing data...\")\n",
        "preprocess_parallel_files(\n",
        "    source_file=en_file,\n",
        "    target_file=gu_file,\n",
        "    output_source=data_dir / \"processed\" / \"en.txt\",\n",
        "    output_target=data_dir / \"processed\" / \"gu.txt\",\n",
        "    min_length=3,\n",
        "    max_length=128,\n",
        "    remove_dup=True\n",
        ")\n",
        "print(\"Preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train/val/test splits\n",
        "print(\"Creating data splits...\")\n",
        "create_data_splits(\n",
        "    source_file=data_dir / \"processed\" / \"en.txt\",\n",
        "    target_file=data_dir / \"processed\" / \"gu.txt\",\n",
        "    output_dir=data_dir / \"splits\",\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    seed=42\n",
        ")\n",
        "print(\"Data splits created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train tokenizers\n",
        "from src.utils.config import load_config\n",
        "from src.tokenization.bpe import BPETokenizer\n",
        "from src.tokenization.unigram import UnigramTokenizer\n",
        "\n",
        "config = load_config(\"config.yaml\")\n",
        "tokenizer_type = config['tokenization']['type']\n",
        "vocab_size = config['tokenization']['vocab_size']\n",
        "splits_dir = Path(config['paths']['splits_dir'])\n",
        "\n",
        "print(f\"Training {tokenizer_type.upper()} tokenizers with vocab_size={vocab_size}...\")\n",
        "\n",
        "if tokenizer_type == \"bpe\":\n",
        "    source_tokenizer = BPETokenizer(vocab_size=vocab_size)\n",
        "    target_tokenizer = BPETokenizer(vocab_size=vocab_size)\n",
        "else:\n",
        "    source_tokenizer = UnigramTokenizer(vocab_size=vocab_size)\n",
        "    target_tokenizer = UnigramTokenizer(vocab_size=vocab_size)\n",
        "\n",
        "# Train source tokenizer\n",
        "source_tokenizer.train([str(splits_dir / \"train.source\")], vocab_size=vocab_size)\n",
        "source_path = splits_dir / f\"source_tokenizer_{tokenizer_type}.json\" if tokenizer_type == \"bpe\" else splits_dir / f\"source_tokenizer_{tokenizer_type}.model\"\n",
        "source_tokenizer.save(source_path)\n",
        "print(f\"Source tokenizer saved to {source_path}\")\n",
        "\n",
        "# Train target tokenizer\n",
        "target_tokenizer.train([str(splits_dir / \"train.target\")], vocab_size=vocab_size)\n",
        "target_path = splits_dir / f\"target_tokenizer_{tokenizer_type}.json\" if tokenizer_type == \"bpe\" else splits_dir / f\"target_tokenizer_{tokenizer_type}.model\"\n",
        "target_tokenizer.save(target_path)\n",
        "print(f\"Target tokenizer saved to {target_path}\")\n",
        "\n",
        "print(f\"\\nSource vocab size: {source_tokenizer.get_vocab_size()}\")\n",
        "print(f\"Target vocab size: {target_tokenizer.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation Complete!\n",
        "\n",
        "Next steps:\n",
        "1. Download the prepared data and tokenizers\n",
        "2. Use notebook `02_train_teacher.ipynb` to train the teacher model\n",
        "3. Use notebook `03_train_student.ipynb` to train the student model with distillation"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
