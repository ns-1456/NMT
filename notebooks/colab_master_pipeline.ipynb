{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Colab Master Pipeline ‚Äî T5-Nano (Python ‚Üí C++)\n",
        "\n",
        "End-to-end pipeline:\n",
        "- Clone repo\n",
        "- Install deps\n",
        "- XLCoST data prep ‚Üí `data/processed/`\n",
        "- Train tokenizer ‚Üí `custom_tokenizer/`\n",
        "- Build T5-Nano (random init)\n",
        "- Train ‚Üí `t5_nano_checkpoints/` + `final_model/`\n",
        "- Inference demo\n",
        "\n",
        "**Colab GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2a1eb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Storage setup (VS Code + Colab compatible) ---\n",
        "# drive.mount() doesn't work in VS Code extension - we save locally\n",
        "# After training, run the \"Upload to Drive\" cell to persist your model\n",
        "\n",
        "DRIVE_MOUNTED = False  # Set to True only if using Colab web UI with drive.mount()\n",
        "print(\"Models will be saved locally to final_model/\")\n",
        "print(\"After training, run the 'Upload to Drive' cell to save to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b38aec39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 0) Clone repo (idempotent) ---\n",
        "%cd /content\n",
        "\n",
        "REPO_URL = \"https://github.com/ns-1456/NMT.git\"\n",
        "REPO_DIR = \"NMT\"\n",
        "BRANCH = \"python-to-cpp-transpiler\"  # change if needed\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    !git clone --depth 1 -b {BRANCH} {REPO_URL} {REPO_DIR}\n",
        "\n",
        "%cd /content/{REPO_DIR} \n",
        "!git status -sb || true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "003f0cce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1) Install deps (avoid reinstalling torch in Colab) ---\n",
        "!pip -q install -U pip\n",
        "!pip -q install transformers datasets tokenizers pandas scikit-learn accelerate gdown tqdm matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9c280d11",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "REPO_ROOT = Path.cwd()\n",
        "RAW_DIR = REPO_ROOT / \"data\" / \"raw\"\n",
        "PROCESSED_DIR = REPO_ROOT / \"data\" / \"processed\"\n",
        "TOKENIZER_DIR = REPO_ROOT / \"custom_tokenizer\"\n",
        "\n",
        "# Save models locally (upload to Drive after training)\n",
        "CHECKPOINT_DIR = REPO_ROOT / \"t5_nano_checkpoints\"\n",
        "FINAL_MODEL_DIR = REPO_ROOT / \"final_model\"\n",
        "\n",
        "QUICK_RUN = False\n",
        "MAX_SAMPLES = 2000 if QUICK_RUN else None\n",
        "EPOCHS = 1 if QUICK_RUN else 30\n",
        "\n",
        "print(f\"repo: {REPO_ROOT}\")\n",
        "print(f\"checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\"final model: {FINAL_MODEL_DIR}\")\n",
        "BATCH_SIZE = 8 if QUICK_RUN else 32\n",
        "\n",
        "print(\"repo:\", REPO_ROOT)\n",
        "print(\"quick_run:\", QUICK_RUN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc0d7236",
      "metadata": {},
      "source": [
        "## 2) Data prep (XLCoST)\n",
        "\n",
        "This downloads + extracts XLCoST and writes:\n",
        "- `data/processed/corpus.txt`\n",
        "- `data/processed/train.jsonl`, `validation.jsonl`, `test.jsonl`\n",
        "- `data/processed/xlcost_py_cpp_snippet/` (Arrow dataset, if `datasets` is installed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "01baa189",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean stale artifacts that commonly cause confusion\n",
        "subprocess.run([\"rm\", \"-rf\", str(RAW_DIR / \"XLCoST_data\")], check=False)\n",
        "subprocess.run([\"rm\", \"-rf\", str(RAW_DIR / \"__MACOSX\")], check=False)\n",
        "subprocess.run([\"rm\", \"-f\", str(RAW_DIR / \"XLCoST_data.zip\")], check=False)\n",
        "\n",
        "cmd = [\"python\", \"-u\", \"data_prep.py\"]\n",
        "if MAX_SAMPLES is not None:\n",
        "    cmd += [\"--max_samples\", str(MAX_SAMPLES)]\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "proc = subprocess.run(cmd, text=True, capture_output=True)\n",
        "\n",
        "print(\"\\n--- data_prep.py stdout ---\\n\")\n",
        "print(proc.stdout)\n",
        "\n",
        "if proc.returncode != 0:\n",
        "    print(\"\\n--- data_prep.py stderr ---\\n\")\n",
        "    print(proc.stderr)\n",
        "    raise RuntimeError(f\"data_prep.py failed with exit code {proc.returncode}\")\n",
        "\n",
        "print(\"\\nProduced:\")\n",
        "for p in sorted(PROCESSED_DIR.glob(\"*\")):\n",
        "    print(\"-\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7c0af2f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity: locate where pair_data_tok_1 ended up (debug helper)\n",
        "import os\n",
        "\n",
        "hits = []\n",
        "for root, dirs, _files in os.walk(RAW_DIR):\n",
        "    if \"pair_data_tok_1\" in dirs:\n",
        "        hits.append(Path(root))\n",
        "\n",
        "print(\"Found roots containing pair_data_tok_1:\")\n",
        "for h in hits[:10]:\n",
        "    print(\"-\", h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "41de7985",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect dataset + basic visualization\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "arrow_dir = PROCESSED_DIR / \"xlcost_py_cpp_snippet\"\n",
        "if arrow_dir.exists():\n",
        "    from datasets import load_from_disk\n",
        "    ds = load_from_disk(str(arrow_dir))\n",
        "    train_df = pd.DataFrame(ds[\"train\"])\n",
        "else:\n",
        "    train_df = pd.read_json(PROCESSED_DIR / \"train.jsonl\", lines=True)\n",
        "\n",
        "print(\"train rows:\", len(train_df))\n",
        "train_df[\"source_len\"] = train_df[\"source\"].astype(str).map(len)\n",
        "train_df[\"target_len\"] = train_df[\"target\"].astype(str).map(len)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax[0].hist(train_df[\"source_len\"], bins=50)\n",
        "ax[0].set_title(\"Train source char length\")\n",
        "ax[1].hist(train_df[\"target_len\"], bins=50)\n",
        "ax[1].set_title(\"Train target char length\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "train_df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3a2550",
      "metadata": {},
      "source": [
        "## 3) Train tokenizer (Byte-Level BPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "158349a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "subprocess.run([\"python\", \"-u\", \"train_tokenizer.py\"], check=True)\n",
        "print(\"Tokenizer dir:\", TOKENIZER_DIR)\n",
        "!ls -la custom_tokenizer | head\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5595f8",
      "metadata": {},
      "source": [
        "## 4) Build T5-Nano + verify parameter count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b6ce537",
      "metadata": {},
      "outputs": [],
      "source": [
        "import model_config\n",
        "\n",
        "tok = model_config.load_tokenizer()\n",
        "model = model_config.build_t5_nano(tok)\n",
        "params = model_config.count_parameters(model)\n",
        "print(f\"T5-Nano parameter count: {params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92cf00bd",
      "metadata": {},
      "source": [
        "## 5) Train\n",
        "\n",
        "`train.py` uses `fp16=True`, so this requires a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c0818e3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "999db184",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model (saves locally, upload to Drive after)\n",
        "!python -u train.py \\\n",
        "    --output_dir \"{CHECKPOINT_DIR}\" \\\n",
        "    --final_model_dir \"{FINAL_MODEL_DIR}\" \\\n",
        "    --per_device_batch_size 32 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --num_train_epochs 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "ab25169b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Upload model to Google Drive (VS Code workaround) ---\n",
        "# Run this after training to persist your model to Drive\n",
        "\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Zip the model\n",
        "model_zip = \"final_model.zip\"\n",
        "if FINAL_MODEL_DIR.exists():\n",
        "    print(f\"Zipping {FINAL_MODEL_DIR}...\")\n",
        "    shutil.make_archive(\"final_model\", \"zip\", FINAL_MODEL_DIR)\n",
        "    print(f\"Created {model_zip}\")\n",
        "    \n",
        "    # Upload using gdown's gdrive (or manually download)\n",
        "    print(\"\\nTo save to Google Drive, either:\")\n",
        "    print(\"1. Download locally: from google.colab import files; files.download('final_model.zip')\")\n",
        "    print(\"2. Or copy to Drive folder if you have it mounted elsewhere\")\n",
        "    print(f\"\\nModel size: {Path(model_zip).stat().st_size / 1024 / 1024:.1f} MB\")\n",
        "else:\n",
        "    print(f\"Model not found at {FINAL_MODEL_DIR}. Train first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ab1ce789",
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls -lh /content\n",
        "!ls -lh /content/NMT\n",
        "!ls -lh /content/NMT/final_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "97df4ad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves (if trainer_state.json exists)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "trainer_states = list(CHECKPOINT_DIR.glob(\"checkpoint-*/trainer_state.json\"))\n",
        "if not trainer_states:\n",
        "    root_state = CHECKPOINT_DIR / \"trainer_state.json\"\n",
        "    trainer_states = [root_state] if root_state.exists() else []\n",
        "\n",
        "if not trainer_states:\n",
        "    print(\"No trainer_state.json found\")\n",
        "else:\n",
        "    state_path = max(trainer_states, key=lambda p: p.stat().st_mtime)\n",
        "    state = json.loads(state_path.read_text())\n",
        "    logs = state.get(\"log_history\", [])\n",
        "\n",
        "    steps, train_losses = [], []\n",
        "    eval_steps, eval_losses = [], []\n",
        "    for item in logs:\n",
        "        if \"loss\" in item and \"eval_loss\" not in item:\n",
        "            steps.append(item.get(\"step\"))\n",
        "            train_losses.append(item[\"loss\"])\n",
        "        if \"eval_loss\" in item:\n",
        "            eval_steps.append(item.get(\"step\"))\n",
        "            eval_losses.append(item[\"eval_loss\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    if train_losses:\n",
        "        plt.plot(steps, train_losses, label=\"train_loss\")\n",
        "    if eval_losses:\n",
        "        plt.plot(eval_steps, eval_losses, label=\"eval_loss\")\n",
        "    plt.title(\"Training curves\")\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f57c7a",
      "metadata": {},
      "source": [
        "## 6) Inference demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "664987b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo cell - run the cell below to see multiple translation examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197ab21a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Cloning repository from https://github.com/ns-1456/NMT.git...\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Clone repo if not already cloned\n",
        "repo_url = \"https://github.com/ns-1456/NMT.git\"\n",
        "repo_dir = Path(\"/content/NMT\")\n",
        "branch = \"python-to-cpp-transpiler\"\n",
        "\n",
        "if not repo_dir.exists():\n",
        "    print(f\"üì• Cloning repository from {repo_url}...\")\n",
        "    os.chdir(\"/content\")\n",
        "    os.system(f\"git clone --depth 1 -b {branch} {repo_url} NMT\")\n",
        "    print(\"‚úÖ Repository cloned\")\n",
        "else:\n",
        "    print(f\"‚úÖ Repository already exists at {repo_dir}\")\n",
        "\n",
        "# Change to the NMT directory\n",
        "os.chdir(repo_dir)\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add repo directory to Python path\n",
        "if str(repo_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_dir))\n",
        "\n",
        "# Check if inference.py exists\n",
        "inference_file = repo_dir / \"inference.py\"\n",
        "if not inference_file.exists():\n",
        "    print(f\"‚ùå Error: inference.py not found at {inference_file}\")\n",
        "    print(f\"Directory contents: {list(repo_dir.iterdir())[:10]}\")\n",
        "    raise FileNotFoundError(f\"inference.py not found at {inference_file}\")\n",
        "\n",
        "# Import inference module\n",
        "import inference\n",
        "print(\"‚úÖ Successfully imported inference module\")\n",
        "\n",
        "# Point inference to the trained model location (use cloud storage if available)\n",
        "try:\n",
        "    # Try to use FINAL_MODEL_DIR from earlier cells\n",
        "    inference.FINAL_MODEL_DIR = FINAL_MODEL_DIR\n",
        "    print(f\"‚úÖ Using FINAL_MODEL_DIR: {FINAL_MODEL_DIR}\")\n",
        "except NameError:\n",
        "    # Default to local final_model directory\n",
        "    inference.FINAL_MODEL_DIR = repo_dir / \"final_model\"\n",
        "    print(f\"‚úÖ Using default FINAL_MODEL_DIR: {inference.FINAL_MODEL_DIR}\")\n",
        "    \n",
        "    # Check if model exists, if not, download from cloud storage\n",
        "    if not inference.FINAL_MODEL_DIR.exists():\n",
        "        print(\"‚ö†Ô∏è  Local model not found. Checking cloud storage...\")\n",
        "        \n",
        "        # Option 1: Download from Google Drive (if you have the file ID)\n",
        "        # Uncomment and set your Google Drive file ID:\n",
        "        # from google.colab import drive\n",
        "        # drive.mount('/content/drive')\n",
        "        # !cp -r /content/drive/MyDrive/path/to/final_model {inference.FINAL_MODEL_DIR}\n",
        "        \n",
        "        # Option 2: Download from a direct URL (if you've uploaded to cloud storage)\n",
        "        # Uncomment and set your model URL:\n",
        "        # import gdown\n",
        "        # model_url = \"YOUR_CLOUD_STORAGE_URL_HERE\"  # e.g., Google Drive shareable link\n",
        "        # gdown.download_folder(model_url, output=str(inference.FINAL_MODEL_DIR), quiet=False)\n",
        "        \n",
        "        # Option 3: Download final_model.zip and extract\n",
        "        model_zip = repo_dir / \"final_model.zip\"\n",
        "        if model_zip.exists():\n",
        "            print(f\"üì¶ Found final_model.zip, extracting...\")\n",
        "            import zipfile\n",
        "            with zipfile.ZipFile(model_zip, 'r') as zip_ref:\n",
        "                zip_ref.extractall(repo_dir)\n",
        "            print(\"‚úÖ Model extracted\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Model not found locally. Please:\")\n",
        "            print(\"   1. Upload final_model.zip to the repo directory, or\")\n",
        "            print(\"   2. Upload final_model/ folder to cloud storage and download it here\")\n",
        "\n",
        "print(\"üöÄ T5-Nano Python ‚Üí C++ Translator Demo\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Example 1: Simple function\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìù Example 1: Sum Function\")\n",
        "print(\"\\n--- Python Input ---\")\n",
        "python_code_1 = \"\"\"def sum_upto(n):\n",
        "    s = 0\n",
        "    for i in range(n + 1):\n",
        "        s += i\n",
        "    return s\"\"\"\n",
        "print(python_code_1)\n",
        "print(\"\\n--- Generated C++ ---\")\n",
        "cpp_1 = inference.translate(python_code_1)\n",
        "\n",
        "# Example 2: List operations\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìù Example 2: Find Maximum\")\n",
        "print(\"\\n--- Python Input ---\")\n",
        "python_code_2 = \"\"\"def find_max(arr):\n",
        "    if not arr:\n",
        "        return None\n",
        "    max_val = arr[0]\n",
        "    for val in arr:\n",
        "        if val > max_val:\n",
        "            max_val = val\n",
        "    return max_val\"\"\"\n",
        "print(python_code_2)\n",
        "print(\"\\n--- Generated C++ ---\")\n",
        "cpp_2 = inference.translate(python_code_2)\n",
        "\n",
        "# Example 3: String manipulation\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìù Example 3: String Reversal\")\n",
        "print(\"\\n--- Python Input ---\")\n",
        "python_code_3 = \"\"\"def reverse_string(s):\n",
        "    result = \"\"\n",
        "    for i in range(len(s) - 1, -1, -1):\n",
        "        result += s[i]\n",
        "    return result\"\"\"\n",
        "print(python_code_3)\n",
        "print(\"\\n--- Generated C++ ---\")\n",
        "cpp_3 = inference.translate(python_code_3)\n",
        "\n",
        "# Example 4: Conditional logic\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìù Example 4: Even Check\")\n",
        "print(\"\\n--- Python Input ---\")\n",
        "python_code_4 = \"\"\"def is_even(n):\n",
        "    if n % 2 == 0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\"\"\"\n",
        "print(python_code_4)\n",
        "print(\"\\n--- Generated C++ ---\")\n",
        "cpp_4 = inference.translate(python_code_4)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚ú® Demo complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
