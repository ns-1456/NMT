{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e1ba5c2",
      "metadata": {},
      "source": [
        "## T5-Nano (Python → C++) Master Pipeline\n",
        "\n",
        "This notebook runs the complete project pipeline:\n",
        "\n",
        "- Data prep (XLCoST snippet-level Python↔C++)\n",
        "- Tokenizer training (Byte-Level BPE, vocab=16k)\n",
        "- Model init (T5-Nano, random weights)\n",
        "- Training (Seq2SeqTrainer)\n",
        "- Inference demo\n",
        "\n",
        "It also includes visualizations at each stage (length histograms, tokenizer stats, training curves).\n",
        "\n",
        "**Assumptions**\n",
        "- Run from the repo root.\n",
        "- Dependencies installed from `requirements.txt`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042b5abc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_ROOT = Path.cwd()\n",
        "DATA_RAW = REPO_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = REPO_ROOT / \"data\" / \"processed\"\n",
        "TOKENIZER_DIR = REPO_ROOT / \"custom_tokenizer\"\n",
        "CHECKPOINT_DIR = REPO_ROOT / \"t5_nano_checkpoints\"\n",
        "FINAL_MODEL_DIR = REPO_ROOT / \"final_model\"\n",
        "\n",
        "# Toggle this to run fast sanity-checks.\n",
        "QUICK_RUN = True\n",
        "MAX_SAMPLES = 2000 if QUICK_RUN else None  # per split cap in data_prep.py\n",
        "EPOCHS = 1 if QUICK_RUN else 30\n",
        "BATCH_SIZE = 8 if QUICK_RUN else 32\n",
        "\n",
        "print(\"repo:\", REPO_ROOT)\n",
        "print(\"quick_run:\", QUICK_RUN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa6cbdf",
      "metadata": {},
      "source": [
        "## 0) (Optional) Install dependencies\n",
        "\n",
        "If you’re in a fresh environment:\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "If you see SSL / certificate issues during install, fix your Python/certificates setup first (macOS frameworks Python sometimes needs certificate install scripts)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cca8b73",
      "metadata": {},
      "source": [
        "## 1) Data prep (XLCoST → `data/processed/`)\n",
        "\n",
        "This runs `data_prep.py` to:\n",
        "- Download + extract XLCoST (snippet-level)\n",
        "- Build `train/validation/test` parallel pairs\n",
        "- Write `corpus.txt` for tokenizer training\n",
        "- Save the Arrow dataset (if `datasets` is installed)\n",
        "- Always export JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6540c494",
      "metadata": {},
      "outputs": [],
      "source": [
        "cmd = [\"python\", \"data_prep.py\"]\n",
        "if MAX_SAMPLES is not None:\n",
        "    cmd += [\"--max_samples\", str(MAX_SAMPLES)]\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"\\nProduced:\")\n",
        "for p in sorted(DATA_PROCESSED.glob(\"*\")):\n",
        "    print(\"-\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80eb90bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset for inspection.\n",
        "# Prefer Arrow dataset (fast); fallback to JSONL if needed.\n",
        "arrow_dir = DATA_PROCESSED / \"xlcost_py_cpp_snippet\"\n",
        "\n",
        "if arrow_dir.exists():\n",
        "    from datasets import load_from_disk\n",
        "\n",
        "    ds = load_from_disk(str(arrow_dir))\n",
        "    train_df = pd.DataFrame(ds[\"train\"])\n",
        "    val_df = pd.DataFrame(ds[\"validation\"]) if \"validation\" in ds else None\n",
        "    test_df = pd.DataFrame(ds[\"test\"]) if \"test\" in ds else None\n",
        "else:\n",
        "    train_df = pd.read_json(DATA_PROCESSED / \"train.jsonl\", lines=True)\n",
        "    val_df = pd.read_json(DATA_PROCESSED / \"validation.jsonl\", lines=True)\n",
        "    test_df = pd.read_json(DATA_PROCESSED / \"test.jsonl\", lines=True)\n",
        "\n",
        "print(\"train rows:\", len(train_df))\n",
        "print(\"val rows:\", len(val_df) if val_df is not None else None)\n",
        "print(\"test rows:\", len(test_df) if test_df is not None else None)\n",
        "train_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ea903e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dataset lengths (character length)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_df[\"source_len\"] = train_df[\"source\"].astype(str).map(len)\n",
        "train_df[\"target_len\"] = train_df[\"target\"].astype(str).map(len)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax[0].hist(train_df[\"source_len\"], bins=50)\n",
        "ax[0].set_title(\"Train source char length\")\n",
        "ax[0].set_xlabel(\"chars\")\n",
        "\n",
        "ax[1].hist(train_df[\"target_len\"], bins=50)\n",
        "ax[1].set_title(\"Train target char length\")\n",
        "ax[1].set_xlabel(\"chars\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(train_df[[\"source_len\", \"target_len\"]].describe(percentiles=[0.5, 0.9, 0.95, 0.99]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "554ac68e",
      "metadata": {},
      "source": [
        "## 2) Train tokenizer (Byte-Level BPE)\n",
        "\n",
        "This runs `train_tokenizer.py` to create:\n",
        "- `custom_tokenizer/vocab.json`\n",
        "- `custom_tokenizer/merges.txt`\n",
        "\n",
        "Then we load it and inspect basic stats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81133055",
      "metadata": {},
      "outputs": [],
      "source": [
        "cmd = [\"python\", \"train_tokenizer.py\"]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"Tokenizer files:\")\n",
        "for p in sorted(TOKENIZER_DIR.glob(\"*\") ):\n",
        "    print(\"-\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9389aaf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import model_config\n",
        "\n",
        "tok = model_config.load_tokenizer()\n",
        "print(\"vocab_size:\", tok.vocab_size)\n",
        "print(\"special tokens:\")\n",
        "print({\n",
        "    \"bos\": tok.bos_token,\n",
        "    \"pad\": tok.pad_token,\n",
        "    \"eos\": tok.eos_token,\n",
        "    \"unk\": tok.unk_token,\n",
        "    \"mask\": tok.mask_token,\n",
        "})\n",
        "\n",
        "sample = train_df.iloc[0][\"source\"]\n",
        "enc = tok(sample)\n",
        "print(\"\\nsample chars:\", len(sample))\n",
        "print(\"sample tokens:\", len(enc[\"input_ids\"]))\n",
        "print(\"first 30 token ids:\", enc[\"input_ids\"][:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4c0b95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token length distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def token_len(s: str) -> int:\n",
        "    return len(tok(s, truncation=False)[\"input_ids\"])\n",
        "\n",
        "lens = train_df[\"source\"].astype(str).head(2000).map(token_len)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(lens, bins=50)\n",
        "plt.title(\"Token length distribution (first 2k sources)\")\n",
        "plt.xlabel(\"tokens\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "print(lens.describe(percentiles=[0.5, 0.9, 0.95, 0.99]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e23de5e",
      "metadata": {},
      "source": [
        "## 3) Build T5-Nano (random init) + verify parameter count\n",
        "\n",
        "This uses `model_config.py` (no pretrained weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a4285e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model_config.build_t5_nano(tok)\n",
        "params = model_config.count_parameters(model)\n",
        "print(f\"T5-Nano parameter count: {params:,}\")\n",
        "print(\"in_expected_range:\", 20_000_000 <= params <= 40_000_000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43e50db",
      "metadata": {},
      "source": [
        "## 4) Train\n",
        "\n",
        "This runs `train.py`.\n",
        "\n",
        "Notes:\n",
        "- `fp16=True` requires a CUDA GPU. If you’re on CPU, edit `train.py` to set `fp16=False`.\n",
        "- In `QUICK_RUN` mode we train for fewer epochs and smaller batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "385b8b27",
      "metadata": {},
      "outputs": [],
      "source": [
        "cmd = [\n",
        "    \"python\",\n",
        "    \"train.py\",\n",
        "    \"--per_device_batch_size\",\n",
        "    str(BATCH_SIZE),\n",
        "    \"--num_train_epochs\",\n",
        "    str(EPOCHS),\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"Final model dir exists:\", FINAL_MODEL_DIR.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6fa3820",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves (train/eval loss) from Trainer state\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "trainer_states = list(CHECKPOINT_DIR.glob(\"checkpoint-*/trainer_state.json\"))\n",
        "if not trainer_states:\n",
        "    # Sometimes Trainer writes trainer_state.json in the root output_dir\n",
        "    root_state = CHECKPOINT_DIR / \"trainer_state.json\"\n",
        "    trainer_states = [root_state] if root_state.exists() else []\n",
        "\n",
        "if not trainer_states:\n",
        "    print(\"No trainer_state.json found yet in\", CHECKPOINT_DIR)\n",
        "else:\n",
        "    # Use the most recent state file\n",
        "    state_path = max(trainer_states, key=lambda p: p.stat().st_mtime)\n",
        "    print(\"Using:\", state_path)\n",
        "\n",
        "    state = json.loads(state_path.read_text())\n",
        "    logs = state.get(\"log_history\", [])\n",
        "\n",
        "    steps, train_losses = [], []\n",
        "    eval_steps, eval_losses = [], []\n",
        "    for item in logs:\n",
        "        if \"loss\" in item and \"eval_loss\" not in item:\n",
        "            steps.append(item.get(\"step\"))\n",
        "            train_losses.append(item[\"loss\"])\n",
        "        if \"eval_loss\" in item:\n",
        "            eval_steps.append(item.get(\"step\"))\n",
        "            eval_losses.append(item[\"eval_loss\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    if train_losses:\n",
        "        plt.plot(steps, train_losses, label=\"train_loss\")\n",
        "    if eval_losses:\n",
        "        plt.plot(eval_steps, eval_losses, label=\"eval_loss\")\n",
        "    plt.title(\"Training curves\")\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0af33dc",
      "metadata": {},
      "source": [
        "## 5) Inference demo\n",
        "\n",
        "This loads from `./final_model` and runs beam search (`num_beams=4`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54400a9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import inference\n",
        "\n",
        "sample_python = \"\"\"\\\n",
        "def factorial(n):\n",
        "    out = 1\n",
        "    for i in range(2, n + 1):\n",
        "        out *= i\n",
        "    return out\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== Python ===\")\n",
        "print(sample_python)\n",
        "print(\"=== C++ (generated) ===\")\n",
        "inference.translate(sample_python)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f70ce6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Make results more visual: sample a few random translations from the validation set\n",
        "import random\n",
        "\n",
        "if val_df is None:\n",
        "    print(\"No validation split loaded\")\n",
        "else:\n",
        "    for idx in random.sample(range(len(val_df)), k=min(3, len(val_df))):\n",
        "        py = val_df.iloc[idx][\"source\"]\n",
        "        # Strip the prefix for prettier display\n",
        "        if py.startswith(TASK_PREFIX):\n",
        "            py = py[len(TASK_PREFIX):]\n",
        "        print(\"\\n--- Example\", idx, \"---\")\n",
        "        print(\"[Python]\")\n",
        "        print(py)\n",
        "        print(\"[Model C++]\")\n",
        "        inference.translate(py)\n",
        "        print(\"[Reference C++]\")\n",
        "        print(val_df.iloc[idx][\"target\"])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
