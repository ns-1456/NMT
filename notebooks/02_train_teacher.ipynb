{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# English-Gujarati NMT: Train Teacher Model\n",
        "\n",
        "This notebook trains the larger teacher model for knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install transformers tokenizers sentencepiece datasets sacrebleu pyyaml tqdm wandb requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load config and tokenizers\n",
        "from src.utils.config import load_config\n",
        "from src.tokenization.bpe import BPETokenizer\n",
        "from src.tokenization.unigram import UnigramTokenizer\n",
        "from src.data.dataset import ParallelDataset, get_dataloader\n",
        "from src.models.transformer import create_teacher_model\n",
        "from src.training.trainer import NMTTrainer\n",
        "from torch.optim import AdamW\n",
        "\n",
        "config = load_config(\"config.yaml\")\n",
        "splits_dir = Path(config['paths']['splits_dir'])\n",
        "tokenizer_type = config['tokenization']['type']\n",
        "\n",
        "# Load tokenizers\n",
        "if tokenizer_type == \"bpe\":\n",
        "    source_tokenizer = BPETokenizer()\n",
        "    target_tokenizer = BPETokenizer()\n",
        "    source_tokenizer.load(splits_dir / f\"source_tokenizer_{tokenizer_type}.json\")\n",
        "    target_tokenizer.load(splits_dir / f\"target_tokenizer_{tokenizer_type}.json\")\n",
        "else:\n",
        "    source_tokenizer = UnigramTokenizer()\n",
        "    target_tokenizer = UnigramTokenizer()\n",
        "    source_tokenizer.load(splits_dir / f\"source_tokenizer_{tokenizer_type}.model\")\n",
        "    target_tokenizer.load(splits_dir / f\"target_tokenizer_{tokenizer_type}.model\")\n",
        "\n",
        "src_vocab_size = source_tokenizer.get_vocab_size()\n",
        "tgt_vocab_size = target_tokenizer.get_vocab_size()\n",
        "\n",
        "print(f\"Source vocab size: {src_vocab_size}\")\n",
        "print(f\"Target vocab size: {tgt_vocab_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create datasets\n",
        "train_dataset = ParallelDataset(\n",
        "    splits_dir / \"train.source\",\n",
        "    splits_dir / \"train.target\",\n",
        "    source_tokenizer,\n",
        "    target_tokenizer,\n",
        "    max_length=config['dataset']['max_length']\n",
        ")\n",
        "\n",
        "val_dataset = ParallelDataset(\n",
        "    splits_dir / \"val.source\",\n",
        "    splits_dir / \"val.target\",\n",
        "    source_tokenizer,\n",
        "    target_tokenizer,\n",
        "    max_length=config['dataset']['max_length']\n",
        ")\n",
        "\n",
        "train_dataloader = get_dataloader(\n",
        "    train_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dataloader = get_dataloader(\n",
        "    val_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_dataloader)}\")\n",
        "print(f\"Val batches: {len(val_dataloader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create teacher model\n",
        "teacher_model = create_teacher_model(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=config['model']['teacher']['d_model'],\n",
        "    nhead=config['model']['teacher']['num_heads'],\n",
        "    num_layers=config['model']['teacher']['num_layers'],\n",
        "    dim_feedforward=config['model']['teacher']['d_ff'],\n",
        "    max_seq_length=config['model']['teacher']['max_seq_length'],\n",
        "    dropout=config['model']['teacher']['dropout'],\n",
        "    pad_token_id=source_tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "teacher_model = teacher_model.to(device)\n",
        "print(f\"Model created and moved to {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup optimizer and trainer\n",
        "optimizer = AdamW(\n",
        "    teacher_model.parameters(),\n",
        "    lr=config['training']['learning_rate'],\n",
        "    weight_decay=config['training']['weight_decay']\n",
        ")\n",
        "\n",
        "trainer = NMTTrainer(\n",
        "    model=teacher_model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    config=config,\n",
        "    teacher_model=None,  # No teacher for teacher training\n",
        "    target_tokenizer=target_tokenizer\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train teacher model\n",
        "print(\"Starting teacher model training...\")\n",
        "trainer.train(num_epochs=config['training']['num_epochs'])\n",
        "\n",
        "print(\"\\nTeacher model training completed!\")\n",
        "print(f\"Best BLEU: {trainer.best_bleu:.2f}\")\n",
        "print(f\"Best model saved to: {Path(config['paths']['checkpoint_dir']) / 'best_model.pt'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Teacher Model Training Complete!\n",
        "\n",
        "**Next Steps:**\n",
        "1. Download the teacher checkpoint from `checkpoints/best_model.pt`\n",
        "2. Update `config.yaml` to set `distillation.teacher_checkpoint` to the teacher checkpoint path\n",
        "3. Use notebook `03_train_student.ipynb` to train the student model with knowledge distillation"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}