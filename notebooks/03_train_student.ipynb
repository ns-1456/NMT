{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# English-Gujarati NMT: Train Student Model with Knowledge Distillation\n",
        "\n",
        "This notebook trains the small student model (<50M params) using knowledge distillation from the teacher model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install transformers tokenizers sentencepiece datasets sacrebleu pyyaml tqdm wandb requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load config and check distillation settings\n",
        "from src.utils.config import load_config\n",
        "\n",
        "config = load_config(\"config.yaml\")\n",
        "\n",
        "# Verify distillation is enabled\n",
        "if not config.get('distillation', {}).get('enabled', False):\n",
        "    print(\"WARNING: Distillation is not enabled in config.yaml!\")\n",
        "    print(\"Set distillation.enabled: true and provide teacher_checkpoint path\")\n",
        "else:\n",
        "    teacher_checkpoint = config['distillation'].get('teacher_checkpoint')\n",
        "    if not teacher_checkpoint or not Path(teacher_checkpoint).exists():\n",
        "        print(\"WARNING: Teacher checkpoint not found!\")\n",
        "        print(f\"Expected at: {teacher_checkpoint}\")\n",
        "        print(\"Please upload the teacher checkpoint or update config.yaml\")\n",
        "    else:\n",
        "        print(f\"Distillation enabled. Teacher checkpoint: {teacher_checkpoint}\")\n",
        "\n",
        "print(f\"\\nDistillation config:\")\n",
        "print(f\"  Enabled: {config.get('distillation', {}).get('enabled', False)}\")\n",
        "print(f\"  Temperature: {config.get('distillation', {}).get('temperature', 4.0)}\")\n",
        "print(f\"  Alpha: {config.get('distillation', {}).get('alpha', 0.5)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load tokenizers\n",
        "from src.tokenization.bpe import BPETokenizer\n",
        "from src.tokenization.unigram import UnigramTokenizer\n",
        "\n",
        "splits_dir = Path(config['paths']['splits_dir'])\n",
        "tokenizer_type = config['tokenization']['type']\n",
        "\n",
        "if tokenizer_type == \"bpe\":\n",
        "    source_tokenizer = BPETokenizer()\n",
        "    target_tokenizer = BPETokenizer()\n",
        "    source_tokenizer.load(splits_dir / f\"source_tokenizer_{tokenizer_type}.json\")\n",
        "    target_tokenizer.load(splits_dir / f\"target_tokenizer_{tokenizer_type}.json\")\n",
        "else:\n",
        "    source_tokenizer = UnigramTokenizer()\n",
        "    target_tokenizer = UnigramTokenizer()\n",
        "    source_tokenizer.load(splits_dir / f\"source_tokenizer_{tokenizer_type}.model\")\n",
        "    target_tokenizer.load(splits_dir / f\"target_tokenizer_{tokenizer_type}.model\")\n",
        "\n",
        "src_vocab_size = source_tokenizer.get_vocab_size()\n",
        "tgt_vocab_size = target_tokenizer.get_vocab_size()\n",
        "\n",
        "print(f\"Source vocab size: {src_vocab_size}\")\n",
        "print(f\"Target vocab size: {tgt_vocab_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create datasets\n",
        "from src.data.dataset import ParallelDataset, get_dataloader\n",
        "\n",
        "train_dataset = ParallelDataset(\n",
        "    splits_dir / \"train.source\",\n",
        "    splits_dir / \"train.target\",\n",
        "    source_tokenizer,\n",
        "    target_tokenizer,\n",
        "    max_length=config['dataset']['max_length']\n",
        ")\n",
        "\n",
        "val_dataset = ParallelDataset(\n",
        "    splits_dir / \"val.source\",\n",
        "    splits_dir / \"val.target\",\n",
        "    source_tokenizer,\n",
        "    target_tokenizer,\n",
        "    max_length=config['dataset']['max_length']\n",
        ")\n",
        "\n",
        "train_dataloader = get_dataloader(\n",
        "    train_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_dataloader = get_dataloader(\n",
        "    val_dataset,\n",
        "    batch_size=config['training']['batch_size'],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_dataloader)}\")\n",
        "print(f\"Val batches: {len(val_dataloader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create student model\n",
        "from src.models.transformer import create_student_model, create_teacher_model\n",
        "\n",
        "student_model = create_student_model(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=config['model']['student']['d_model'],\n",
        "    nhead=config['model']['student']['num_heads'],\n",
        "    num_layers=config['model']['student']['num_layers'],\n",
        "    dim_feedforward=config['model']['student']['d_ff'],\n",
        "    max_seq_length=config['model']['student']['max_seq_length'],\n",
        "    dropout=config['model']['student']['dropout'],\n",
        "    pad_token_id=source_tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "student_model = student_model.to(device)\n",
        "print(f\"Student model created and moved to {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load teacher model for distillation\n",
        "teacher_model = None\n",
        "if config.get('distillation', {}).get('enabled', False):\n",
        "    teacher_checkpoint_path = config['distillation'].get('teacher_checkpoint')\n",
        "    if teacher_checkpoint_path and Path(teacher_checkpoint_path).exists():\n",
        "        print(f\"Loading teacher model from {teacher_checkpoint_path}...\")\n",
        "        checkpoint = torch.load(teacher_checkpoint_path, map_location=device)\n",
        "        \n",
        "        teacher_model = create_teacher_model(\n",
        "            src_vocab_size=src_vocab_size,\n",
        "            tgt_vocab_size=tgt_vocab_size,\n",
        "            d_model=config['model']['teacher']['d_model'],\n",
        "            nhead=config['model']['teacher']['num_heads'],\n",
        "            num_layers=config['model']['teacher']['num_layers'],\n",
        "            dim_feedforward=config['model']['teacher']['d_ff'],\n",
        "            max_seq_length=config['model']['teacher']['max_seq_length'],\n",
        "            dropout=config['model']['teacher']['dropout'],\n",
        "            pad_token_id=source_tokenizer.pad_token_id\n",
        "        )\n",
        "        teacher_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        teacher_model = teacher_model.to(device)\n",
        "        teacher_model.eval()\n",
        "        print(\"Teacher model loaded successfully!\")\n",
        "    else:\n",
        "        print(\"WARNING: Teacher checkpoint not found. Training without distillation.\")\n",
        "else:\n",
        "    print(\"Distillation disabled. Training student model normally.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup optimizer and trainer\n",
        "from torch.optim import AdamW\n",
        "from src.training.trainer import NMTTrainer\n",
        "\n",
        "optimizer = AdamW(\n",
        "    student_model.parameters(),\n",
        "    lr=config['training']['learning_rate'],\n",
        "    weight_decay=config['training']['weight_decay']\n",
        ")\n",
        "\n",
        "trainer = NMTTrainer(\n",
        "    model=student_model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    config=config,\n",
        "    teacher_model=teacher_model,\n",
        "    target_tokenizer=target_tokenizer\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")\n",
        "if teacher_model:\n",
        "    print(\"Knowledge distillation enabled!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train student model\n",
        "print(\"Starting student model training...\")\n",
        "trainer.train(num_epochs=config['training']['num_epochs'])\n",
        "\n",
        "print(\"\\nStudent model training completed!\")\n",
        "print(f\"Best BLEU: {trainer.best_bleu:.2f}\")\n",
        "print(f\"Best model saved to: {Path(config['paths']['checkpoint_dir']) / 'best_model.pt'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Student Model Training Complete!\n",
        "\n",
        "**Next Steps:**\n",
        "1. Download the student checkpoint\n",
        "2. Use notebook `04_evaluate_and_report.ipynb` to evaluate models and generate reports"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}