{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# English-Gujarati NMT: Evaluation and Report Generation\n",
        "\n",
        "This notebook evaluates the trained models and generates comprehensive reports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install transformers tokenizers sentencepiece datasets sacrebleu pyyaml tqdm wandb requests matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate student model\n",
        "from src.utils.config import load_config\n",
        "import sys\n",
        "import os\n",
        "\n",
        "config = load_config(\"config.yaml\")\n",
        "checkpoint_dir = Path(config['paths']['checkpoint_dir'])\n",
        "\n",
        "# Find student checkpoint\n",
        "student_checkpoint = checkpoint_dir / \"best_model.pt\"\n",
        "if student_checkpoint.exists():\n",
        "    print(\"Evaluating student model on test set...\")\n",
        "    # Save original argv\n",
        "    original_argv = sys.argv.copy()\n",
        "    try:\n",
        "        sys.argv = ['evaluate.py', '--checkpoint', str(student_checkpoint), '--split', 'test']\n",
        "        from scripts.evaluate import main\n",
        "        main()\n",
        "    finally:\n",
        "        # Restore original argv\n",
        "        sys.argv = original_argv\n",
        "else:\n",
        "    print(f\"Student checkpoint not found at {student_checkpoint}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate translation examples\n",
        "if 'student_checkpoint' in locals() and student_checkpoint.exists():\n",
        "    print(\"\\nGenerating translation examples...\")\n",
        "    original_argv = sys.argv.copy()\n",
        "    try:\n",
        "        sys.argv = ['translate_examples.py', '--checkpoint', str(student_checkpoint), '--num-examples', '10']\n",
        "        from scripts.translate_examples import main\n",
        "        main()\n",
        "    finally:\n",
        "        sys.argv = original_argv\n",
        "else:\n",
        "    print(\"Skipping translation examples (checkpoint not found)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate comprehensive report\n",
        "from scripts.generate_report import generate_report\n",
        "\n",
        "print(\"Generating evaluation report...\")\n",
        "report = generate_report(\"config.yaml\")\n",
        "print(\"\\nReport generated successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize results\n",
        "print(\"Generating visualizations...\")\n",
        "original_argv = sys.argv.copy()\n",
        "try:\n",
        "    sys.argv = ['visualize_results.py']\n",
        "    from scripts.visualize_results import main\n",
        "    main()\n",
        "finally:\n",
        "    sys.argv = original_argv\n",
        "print(\"\\nVisualizations generated!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare teacher and student models (if both exist)\n",
        "teacher_ckpt = config.get('distillation', {}).get('teacher_checkpoint')\n",
        "if teacher_ckpt and Path(teacher_ckpt).exists() and 'student_checkpoint' in locals() and student_checkpoint.exists():\n",
        "    print(\"\\nComparing teacher and student models...\")\n",
        "    original_argv = sys.argv.copy()\n",
        "    try:\n",
        "        sys.argv = [\n",
        "            'model_comparison.py',\n",
        "            '--teacher-checkpoint', str(teacher_ckpt),\n",
        "            '--student-checkpoint', str(student_checkpoint),\n",
        "            '--split', 'test'\n",
        "        ]\n",
        "        from scripts.model_comparison import main\n",
        "        main()\n",
        "    finally:\n",
        "        sys.argv = original_argv\n",
        "else:\n",
        "    print(\"Skipping model comparison (need both teacher and student checkpoints)\")\n",
        "    if not teacher_ckpt:\n",
        "        print(\"  - Teacher checkpoint path not set in config\")\n",
        "    elif not Path(teacher_ckpt).exists():\n",
        "        print(f\"  - Teacher checkpoint not found at: {teacher_ckpt}\")\n",
        "    elif 'student_checkpoint' not in locals() or not student_checkpoint.exists():\n",
        "        print(f\"  - Student checkpoint not found at: {student_checkpoint if 'student_checkpoint' in locals() else 'N/A'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Complete!\n",
        "\n",
        "**Generated Files:**\n",
        "- `checkpoints/evaluation_report.json` - Comprehensive evaluation report\n",
        "- `checkpoints/predictions_test.txt` - Test set predictions\n",
        "- `checkpoints/translation_examples.txt` - Sample translations\n",
        "- `checkpoints/training_curves.png` - Training visualization\n",
        "- `checkpoints/model_comparison.png` - Model comparison charts\n",
        "- `checkpoints/model_comparison.json` - Detailed comparison metrics\n",
        "\n",
        "**Resume Metrics:**\n",
        "Check the evaluation report for:\n",
        "- Model size reduction percentage\n",
        "- BLEU score retention\n",
        "- Parameter counts\n",
        "- Performance metrics"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}